# TODO

* Keep belief updating animation slides?
* Include modeling in constraint study?

# Notes for slides

## WHere do I want to get with this?

Distributional learning isn't just for acquisition.  Happens throughout life, at
multiple levels.

In fact, the kind of world we inhabit _requires_ lifelong distributional
learning.

## Skeleton

Intro/motivation: perception in a noisy world, is inference under uncertainty.
Inference needs distributions: infer explanation of an observed stimulus/cue
value by working backward, asking how likely that particular stimulus would be
under that explanation (likelihood, in bayesian terms).  When distributions
change, there's not a single "correct" set of distributions to learn.  need
_contnuous distributional learning_. ( slides on recalibration )


Now we have a puzzle: I've proposed that the same process that explains language
_acquisition_ explains _adaptation_ to talker variability.  But learning a
language is _really really hard_, and adapting to unfamiliar talkers is so easy
we only rarely notice that we're doing it.  What gives?

the key to this came to be by thinking about distributional learning as itself
a sort of _inference under uncertainty_ ( Bayes )

we can think of Distr. learning as a process of inference under uncertainty at
_another level_, or _hierarchical inference_.  two source of information: data
itself, and _prior expectations_.  ( talker is point in distribution space.
bunch of talkers, learn distribution, rule out a huge number of possibilities )


( slide on adapt/generalize/recognize, to
motivate how good priors help )

Where do these come from? ( god slide ) Experience with other talkers.

( talker's cue distributions -> point in cue space -> distribution of talkers ->
clusters of talkers; lots of clusters you _could_ learn, figuring out which
clusters are most informative is itself an inference process. make explicit
analogy: learn clusters of sounds so you can classify new sounds; learn clusters
of talkers so you can adapt to new talkers )

One prediction: if you've learned the distribution of talkers, even at the most
general level, if you know nothing else about a talker except that they're
speaking English, that still really narrows down the range of distributions you
might expect.  And, as a result, you're much more efficient than when you're
learning the language for the first time.  But the tradeoff is that if you _do_
encounter someone outside the range of what you've experienced (plus some room
for error), you're going to have a hard time adapting.  What this predicts is
that even very rapid distributional learning should be _constrained_ by range of
cue distributions in the language.



Why this matters: Something about prediction?  Integrating perception, learning,
and memory.

## Framing 

Not just lang/dist learning people.  SO need to go easy.  Get into it with
something like

* _Why_ is distributional learning important?  Why do we care about
  distributions?
    * Standard answer for speech: it's how you learn the structure of your
      language from unlabeled input.
    * Another reason: inference.  tradition of "ideal observer" models: when the
      signals from the world are _noisy_ and _ambiguous_, you need to know the
      distribution of signals generated by different possible states of the
      world.

Could even start off by _raising_ the question, then adjust standard slides to
answer that.  e.g. "here's the first reason that we should care about
distributions: if we know what the distributions of these cues are, we can work
backwards to _infer_ how likely each category is as an explanation of an
particular cue value we see".


## ALternative framing

Make it about prediction (more like Kavli) to better tie in with cog neuro
people.  We think the brain is a "prediction engine".  Inference as prediction
(and statistically optimal solution to a noisy, ambiguous world).  Making good
predictions requires knowing the distributions.  **That's** why distributions
matter (can raise this question to start, too: "you're all at this symposium
about distributional learning, but I want to start by questioning the entire
premise: **why** should we (or the brain) care about distributions at all?").

**OR**: we come back to prediction/neural stuff at the end.  "you might be
wondering what the connection to brain stuff is.  well...inference"


## Misc. things

As you're perceiving speech, you're making inferences at three levels: what the
talker is saying, how they say things (their own, particular cue distributions),
and _who_ they are (where they fit in the space of all talkers)




Connecting dist learning of categories in acquisition with dist learning of
socio-indexical groups: in order to extrapolate beyond _particular_ experiences,
you need to (explicitly or implicitly) infer some kind of category structure,
detect some clusters on top of the individual points.


One possible example here: what if you want to continue to track a particular
talker's distributions over multiple encounters?


## Empirical things to discuss

* Rapid recalibration: belief updating is a good description of a pretty
  low-level process.
* Dist learning/prior constraints: range of prior experience matters (-ish)
